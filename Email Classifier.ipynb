{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving the Mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import requests\n",
    "\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If modifying these scopes, delete the file token.json.\n",
    "SCOPES = [\"https://www.googleapis.com/auth/gmail.readonly\"]\n",
    "session=requests.session()\n",
    "\n",
    "def save_html(response, path):\n",
    "    html_content = response.content\n",
    "    with open(path, 'wb') as html_file:\n",
    "        html_file.write(html_content)\n",
    "    print(f\"HTML content saved to '{path}'\")\n",
    "\n",
    "def authenticate():\n",
    "    creds = None\n",
    "    if os.path.exists(\"token.json\"):\n",
    "        print(\"User token found\")\n",
    "        creds = Credentials.from_authorized_user_file(\"token.json\", SCOPES)\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\"client_secret.json\", SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        with open(\"token.json\", \"w\") as token:\n",
    "            token.write(creds.to_json())\n",
    "    print(\"Authenticated\")\n",
    "    return creds\n",
    "\n",
    "def get_message_ids():\n",
    "    global creds\n",
    "    global parameters\n",
    "    message_ids = []\n",
    "    while True:\n",
    "        # Move across pages\n",
    "        response_message_getter= session.get(message_url, params=parameters, headers={'Authorization': f'Bearer {creds.token}'})\n",
    "        save_html(response_message_getter, 'json.html')\n",
    "\n",
    "        # Taking out messages on each page\n",
    "        messages = response_message_getter.json().get('messages', [])\n",
    "        print(len(messages))\n",
    "\n",
    "        next_page_token = response_message_getter.json().get('nextPageToken')\n",
    "\n",
    "        print('messages\\n', messages, '\\nnext page token\\n', next_page_token)\n",
    "        with open ('message_json.txt', 'a') as f:\n",
    "            for message in messages:\n",
    "                message_ids.append(message['id'] )\n",
    "                f.write(f'{message}\\n')\n",
    "        if next_page_token:\n",
    "            parameters['pageToken'] = next_page_token\n",
    "            print(parameters)\n",
    "        else:\n",
    "            break\n",
    "    print('writing to ids file')\n",
    "    with open ('message_ids.txt', 'a') as f:\n",
    "        for message in message_ids:\n",
    "            f.write(f'{message}\\n')\n",
    "    return message_ids\n",
    "\n",
    "def get_a_message(message_id):\n",
    "    global message_count\n",
    "    message_count += 1\n",
    "    print(f\"Retrieved {message_count} messages.\")\n",
    "    url = f'https://gmail.googleapis.com/gmail/v1/users/{userId}/messages/{message_id}'\n",
    "    params = {'format': 'full'}\n",
    "    response = requests.get(url, params=params, headers={'Authorization': f'Bearer {creds.token}'})\n",
    "    save_html(response, f'root/messages/message{message_id}.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#main loop\n",
    "creds = authenticate()\n",
    "with open('message_ids.txt', 'w') as f:\n",
    "    f.close()\n",
    "with open('message_json.txt', 'w') as f:\n",
    "    f.close()\n",
    "message_count=10\n",
    "try:\n",
    "    service = build('gmail', 'v1', credentials=creds)\n",
    "    userId = 'arehman.bee22seecs@seecs.edu.pk'\n",
    "    message_url = f'https://gmail.googleapis.com/gmail/v1/users/{userId}/messages'\n",
    "    parameters = {'maxResults': 500}\n",
    "    message_ids=get_message_ids()\n",
    "    print(len(message_ids))\n",
    "    print(\"done\")\n",
    "    threads = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures=[executor.submit(get_a_message, message_id) for message_id in message_ids]\n",
    "        for future in as_completed(futures):\n",
    "            print(future.result())\n",
    "\n",
    "except HttpError as error:\n",
    "    print(f\"An error occurred: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hnVX5IkPMZLZ"
   },
   "source": [
    "# **Pre-processing Raw Mail response**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ScF5UbQJMgCg"
   },
   "source": [
    "## MIME to text level processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bpH2na7yM4Kl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#stats for summarizing mime types\n",
    "total_messages_viewed=0\n",
    "others=[]\n",
    "single_part_count=0\n",
    "multi_part_count=0\n",
    "plain_text_count=0\n",
    "text_html_count=0\n",
    "others_count=0\n",
    "\n",
    "to_decode_plain_text=[]\n",
    "to_decode_html=[]\n",
    "#payload->headers->subject\n",
    "os.makedirs(\"root/messages/Distilled\", exist_ok=True)\n",
    "def print_text_body(path):\n",
    "    with open(f\"root/messages/Distilled/{path.split('/')[2]}\", 'w', encoding='utf-8') as f:\n",
    "        pass\n",
    "    global single_part_count,multi_part_count,plain_text_count,text_html_count,others_count,total_messages_viewed,to_decode_plain_text,to_decode_html\n",
    "\n",
    "    total_messages_viewed+=1\n",
    "    to_decode_html = []#emptying the dictionary before populating it\n",
    "    print(\"PATHHHHHHHHHHHHHHH\",path)\n",
    "    with open(path,'r',encoding='utf-8') as msg:\n",
    "        message=json.load(msg)\n",
    "\n",
    "\n",
    "    #adding the subject\n",
    "    subject_found = False  # Initialize flag variable\n",
    "    for header in message['payload']['headers']:\n",
    "        if header['name'] == 'Subject':\n",
    "            text = header['value']\n",
    "            print(\"Subject:\", text)\n",
    "            with open(f\"root/messages/Distilled/{path.split('/')[2]}\", 'a', encoding='utf-8') as f:\n",
    "                f.write(text)\n",
    "            subject_found = True  # Set flag to True if subject header is found\n",
    "    # Check flag and print message if subject is not found\n",
    "    if not subject_found:\n",
    "        print(\"No subject for this one:\", path.split('/')[2])\n",
    "\n",
    "    #multipart case\n",
    "    if 'parts' in message['payload']:\n",
    "        multi_part_count+=1\n",
    "        for part in message['payload']['parts']:\n",
    "            process_a_single_part(part)\n",
    "    #single part case\n",
    "    else:\n",
    "        single_part_count+=1\n",
    "        part=message['payload']\n",
    "        process_a_single_part(part)\n",
    "    #decode and save html part\n",
    "    for encoded_html in to_decode_html:\n",
    "        html= base64.urlsafe_b64decode(encoded_html).decode('utf-8')\n",
    "        soup_object= BeautifulSoup(html,'html.parser')\n",
    "        text=soup_object.get_text(separator='\\n', strip=True)\n",
    "        with open(f\"root/messages/Distilled/{path.split('/')[2]}\",'a',encoding='utf-8') as f:\n",
    "            f.write(text)\n",
    "\n",
    "    #decode and save plain text part\n",
    "    msg_str=\"\"\n",
    "    to_decode_plain_text=[]#empty the dictionary\n",
    "    for encoded in to_decode_plain_text:\n",
    "        msg_str+=base64.urlsafe_b64decode(encoded).decode('utf-8')\n",
    "    with open(f\"root/messages/Distilled/{path.split('/')[2]}\", 'a', encoding='utf-8') as f:\n",
    "        f.write(\"\\n*************************************************************************\\n\\n\\n\\n*************************************************************************\")\n",
    "        f.write(msg_str)\n",
    "    print\n",
    "def process_a_single_part(part):\n",
    "    global single_part_count, multi_part_count, plain_text_count, text_html_count, others_count, total_messages_viewed, to_decode_plain_text\n",
    "    if part['mimeType'].startswith('text/'):\n",
    "        print(part['mimeType'])\n",
    "        #print(part['body'])#commented out cuz big mess in CLI\n",
    "        if part['mimeType'].endswith('html'):#include both amp and regular html\n",
    "            if 'data' in part['body']:\n",
    "                to_decode_html.append(part['body']['data'])\n",
    "            text_html_count += 1\n",
    "        elif part['mimeType'].endswith('plain'):\n",
    "            if 'data' in part['body']:\n",
    "                to_decode_plain_text.append(part['body']['data'])\n",
    "            plain_text_count += 1\n",
    "        else:\n",
    "            others_count += 1\n",
    "            others.append(part['mimeType'])\n",
    "\n",
    "files_stack=[]\n",
    "for file in os.listdir('root/messages'):\n",
    "        if file.endswith('json'):\n",
    "          print(file,\":******************************************************************************\")\n",
    "          print_text_body('root/messages/'+file)\n",
    "\n",
    "\n",
    "print(\"Total number of messages viewed: \",total_messages_viewed)\n",
    "\n",
    "print(\"Single part messages count: \",single_part_count)\n",
    "print(\"Multi part messages count: \",multi_part_count)\n",
    "\n",
    "print(\"Messages with plain text: \", plain_text_count)\n",
    "print(\"Messages with html text: \", text_html_count)\n",
    "\n",
    "print(\"Messages with text but not html or plain: \",others_count)\n",
    "print(\"their list: \",others)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIeMgjJhM9rB"
   },
   "source": [
    "## Text Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZSW6ZwJM7bk"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import os\n",
    "import re\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "model2 = api.load('word2vec-google-news-300')\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize NLTK objects\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define the directory containing distilled text data\n",
    "distilled_dir = \"root/messages/Distilled\"\n",
    "\n",
    "# Create the output directory for preprocessed data\n",
    "output_dir = \"Corpus\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define a function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    content = text.split()\n",
    "    filtered_content = [word for word in content if word in model2]\n",
    "    textf1=\" \".join(filtered_content)\n",
    "    textf1 = re.sub(r'[^a-zA-Z\\s]', ' ', textf1)\n",
    "    # Reduce multiple whitespaces to a single whitespace\n",
    "    textf1 = re.sub(r'\\s+', ' ', textf1)\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(textf1)\n",
    "    # Remove stopwords and perform lemmatization\n",
    "    preprocessed_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.lower() not in stop_words]\n",
    "    # Join the tokens back into a string\n",
    "    preprocessed_text = ' '.join(preprocessed_tokens)\n",
    "    print(preprocessed_text)\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wBZzlcDJNPEP"
   },
   "outputs": [],
   "source": [
    "# Process each file in the Distilled directory\n",
    "whole_corpus=''\n",
    "# Define a list to store filenames with empty preprocessed contents\n",
    "empty_files = []\n",
    "\n",
    "# Process each file in the Distilled directory\n",
    "for file in os.listdir(distilled_dir):\n",
    "    with open(os.path.join(distilled_dir, file), encoding=\"utf-8\") as f:\n",
    "        print(\"Processing file:\", file)\n",
    "        contents = f.read()\n",
    "        # Perform preprocessing on the contents\n",
    "        preprocessed_contents = preprocess_text(contents)\n",
    "        # Check if the preprocessed content is empty\n",
    "        if not preprocessed_contents.strip():\n",
    "            # If the preprocessed content is empty, note the filename\n",
    "            empty_files.append(file)\n",
    "            print(\"Empty content for file:\", file)\n",
    "            continue  # Skip saving the file\n",
    "        # Write the preprocessed contents to a new file in the output directory\n",
    "        output_file_path = os.path.join(output_dir, file)\n",
    "        with open(output_file_path, 'w', encoding=\"utf-8\") as output_file:\n",
    "            output_file.write(preprocessed_contents)\n",
    "            whole_corpus+=preprocessed_contents\n",
    "        print(\"Preprocessed contents saved to:\", output_file_path)\n",
    "with open(os.path.join(output_dir,'The_whole_corpus.txt'),'w',encoding='utf-8')as f:\n",
    "  f.write(whole_corpus)\n",
    "\n",
    "# Print count of files with empty preprocessed contents and their names\n",
    "print(\"Number of files with empty preprocessed contents:\", len(empty_files))\n",
    "print(\"File names with empty preprocessed contents:\", empty_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "model2 = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Document made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "mails=[]\n",
    "for file in os.listdir('Corpus'):\n",
    "    if file!='The_whole_corpus.txt':\n",
    "        with open(f\"Corpus/{(file)}\",'r',encoding='utf-8') as f:\n",
    "            mails.append(f.read())\n",
    "            print(\"added: \",file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Create CountVectorizer for BoW representation\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the corpus\n",
    "bow_matrix = count_vectorizer.fit_transform(mails)\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print BoW matrix\n",
    "print(bow_matrix.toarray())\n",
    "\n",
    "# Print feature names\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Thx3fZnNs9D"
   },
   "source": [
    "# **Aggregation for K-means**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSnT66Usoz2q"
   },
   "source": [
    "## Using word to vec summation and average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3DAV304NwA8"
   },
   "outputs": [],
   "source": [
    "def sum_and_average_embeddings_of_a_sentence(mail):\n",
    "    words = mail.split()\n",
    "    words=list(set(words))\n",
    "    embedding_sum = np.zeros(model2.vector_size)\n",
    "    misses=0\n",
    "    for word in words:\n",
    "        if word in model2:\n",
    "            embedding_sum += model2[word]\n",
    "        else:\n",
    "            #print(f\"Word '{word}' not in vocabulary.\")\n",
    "            misses+=1\n",
    "    print(\"words missed %:\",misses*100/len(words))\n",
    "    return embedding_sum / (len(words)-misses),embedding_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bZHj8LbqKTWD"
   },
   "outputs": [],
   "source": [
    "sum=[]\n",
    "average=[]\n",
    "\n",
    "directory = 'Corpus'\n",
    "for file in os.listdir(directory):\n",
    "    if file != 'The_whole_corpus.txt':\n",
    "        with open(os.path.join(directory, file), 'r', encoding='utf-8') as f:\n",
    "            a,s=sum_and_average_embeddings_of_a_sentence(f.read())\n",
    "            sum.append(s)\n",
    "            average.append(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvD_axbGvlUx"
   },
   "source": [
    "## Using tf-idf weighted average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EfKA_m4i4GN9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Read documents from files in the 'Corpus' directory\n",
    "docs = []\n",
    "directory = 'Corpus'\n",
    "for file in os.listdir(directory):\n",
    "    if file != 'The_whole_corpus.txt':\n",
    "        with open(os.path.join(directory, file), 'r', encoding='utf-8') as f:\n",
    "            docs.append(f.read())\n",
    "print(len(docs))\n",
    "# Initialize TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Compute TF-IDF scores for the documents\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(docs)\n",
    "\n",
    "# Retrieve vocabulary and IDF scores\n",
    "vocab = tfidf_vectorizer.get_feature_names_out()\n",
    "idf_scores = tfidf_vectorizer.idf_\n",
    "\n",
    "# Create a dictionary to map words to their indices in vocab\n",
    "word_to_index = {word: index for index, word in enumerate(vocab)}\n",
    "\n",
    "# Compute TF-IDF weighted average of word vectors for each document\n",
    "document_vectors = []\n",
    "zero_count=0\n",
    "for doc in docs:\n",
    "    # Tokenize document\n",
    "    words = doc.split()\n",
    "    # Initialize document vector\n",
    "    doc_vector = np.zeros(300)  # Assume word vector dimension is 300\n",
    "    for word in words:\n",
    "        if word in model2:\n",
    "            # Retrieve word vector\n",
    "            word_vector = model2[word]\n",
    "            # Compute TF-IDF score\n",
    "            index = word_to_index.get(word)\n",
    "            tfidf_score = idf_scores[index] if index is not None else 0.0\n",
    "            # Weighted summation\n",
    "            doc_vector += word_vector * tfidf_score\n",
    "    # Check if document vector is not a zero vector\n",
    "    if np.count_nonzero(doc_vector) > 0:\n",
    "        # Normalize document vector\n",
    "        doc_vector /= np.linalg.norm(doc_vector)\n",
    "    else:\n",
    "        # If the document vector is a zero vector, append it without normalization\n",
    "        print(\"Warning: Zero vector encountered for document.\")\n",
    "        zero_count+=1\n",
    "    # Add document vector to list\n",
    "    document_vectors.append(doc_vector)\n",
    "\n",
    "\n",
    "# Print document vectors\n",
    "for i, vec in enumerate(document_vectors):\n",
    "    print(f\"Document {i+1} vector: {vec}\")\n",
    "print(\"Zero_count=\",zero_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRuKka7Owzhi"
   },
   "source": [
    "## Using Doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GqZek0MW6XMx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "email_dir = \"Corpus\"\n",
    "\n",
    "# List to store TaggedDocuments\n",
    "tagged_data = []\n",
    "\n",
    "# Iterate over each email file in the directory\n",
    "for filename in os.listdir(email_dir):\n",
    "    if filename != \"The_whole_corpus.txt\":\n",
    "        filepath = os.path.join(email_dir, filename)\n",
    "\n",
    "        # Read the contents of the email file\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "            preprocessed_text = file.read()\n",
    "\n",
    "        print(\"Filename:\", filename)\n",
    "        print(\"Preprocessed Text:\", preprocessed_text)\n",
    "        print(\"------------------------------------------\")\n",
    "\n",
    "        # Create a TaggedDocument with the preprocessed text and filename as tag\n",
    "        tagged_doc = TaggedDocument(words=preprocessed_text, tags=[filename])\n",
    "\n",
    "        # Append the TaggedDocument to the list\n",
    "        tagged_data.append(tagged_doc)\n",
    "\n",
    "# Now you have a list of TaggedDocuments, each with the filename as its tag\n",
    "# You can use this tagged_data to train your Doc2Vec model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2U0UEqXKTWF"
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "vector_size = 300  # Dimensionality of document vectors\n",
    "window = 100  # Maximum distance between the current and predicted word\n",
    "min_count = 1  # Ignore words with total frequency lower than this\n",
    "epochs = 20 # Number of iterations over the corpus\n",
    "# Initialize and train the Doc2Vec model using tagged_data\n",
    "model = Doc2Vec(tagged_data, vector_size=vector_size, window=window, min_count=min_count, epochs=epochs)\n",
    "\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"doc2vec_model\")\n",
    "\n",
    "# Alternatively, if you want to load a pre-trained model\n",
    "# model = Doc2Vec.load(\"doc2vec_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gC6sJKSaKTWF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Function to retrieve nearest neighbors for a given document\n",
    "def retrieve_nearest_neighbors(doc_id, model, top_n=3):\n",
    "    # Infer the vector representation of the query document\n",
    "    query_vector = model.dv[doc_id]\n",
    "\n",
    "    # Calculate cosine similarity between the query vector and all other document vectors\n",
    "    similarities = cosine_similarity([query_vector], model.dv.vectors)\n",
    "\n",
    "    # Get the indices of the top similar documents\n",
    "    top_similar_indices = similarities.argsort()[0][-top_n-1:-1][::-1]  # Exclude the query document\n",
    "\n",
    "    # Retrieve the filenames of the top similar documents\n",
    "    top_similar_docs = [model.dv.index_to_key[idx] for idx in top_similar_indices]\n",
    "\n",
    "    return top_similar_docs\n",
    "\n",
    "# Example of how to retrieve nearest neighbors for a document\n",
    "# Choose a document index from the corpus\n",
    "query_doc_index = 23 # Adjust as needed\n",
    "\n",
    "# Retrieve the nearest neighbors for the chosen document\n",
    "nearest_neighbors = retrieve_nearest_neighbors(query_doc_index, model)\n",
    "\n",
    "# Print the filenames of the nearest neighbors\n",
    "print(\"Nearest neighbors for document:\", model.dv.index_to_key[query_doc_index])\n",
    "with open(f\"Corpus/{(model.dv.index_to_key[query_doc_index])}\",'r')as f:\n",
    "        print(f.read())\n",
    "for idx, neighbor in enumerate(nearest_neighbors, 1):\n",
    "    print(f\"{idx}. {neighbor}\")\n",
    "    with open(f\"Corpus/{(neighbor)}\",'r')as f:\n",
    "        print(f.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95-Y7Cc2KTWF"
   },
   "source": [
    "# Applying Kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbsGKkK_KTWF"
   },
   "source": [
    "## Doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gKbaHedCKTWF"
   },
   "outputs": [],
   "source": [
    "document_embeddings=[]\n",
    "for i in range(len(model.dv)):\n",
    "    document_embeddings.append(model.dv[i])\n",
    "#print(document_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FqFvhnwdmoWQ"
   },
   "source": [
    "### n_int=10, max_iter50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9tG6ctprKTWF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "min_clusters = 6\n",
    "max_clusters = 50\n",
    "range_of_clusters = range(min_clusters, max_clusters + 1)\n",
    "\n",
    "silhouette_scores = []\n",
    "wcss_values = []\n",
    "\n",
    "# Perform clustering for each number of clusters in the range\n",
    "for num_clusters in range_of_clusters:\n",
    "    print(num_clusters)\n",
    "    # Initialize K-means clustering with the current number of clusters\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42,n_init=10,max_iter=50)\n",
    "\n",
    "    # Perform clustering on the document embeddings\n",
    "    cluster_labels = kmeans.fit_predict(document_embeddings)\n",
    "\n",
    "    # Compute silhouette score\n",
    "    silhouette_avg = silhouette_score(document_embeddings, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "    # Compute Within-Cluster Sum of Squares (WCSS)\n",
    "    wcss = kmeans.inertia_\n",
    "    wcss_values.append(wcss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot Silhouette Score\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range_of_clusters, silhouette_scores, marker='o')\n",
    "plt.title('Silhouette Score vs. Number of Clusters')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.xticks(range(min_clusters, max_clusters + 1, 2))\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot WCSS values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range_of_clusters, wcss_values, marker='o')\n",
    "plt.title('Within-Cluster Sum of Squares (WCSS) vs. Number of Clusters')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('WCSS')\n",
    "plt.xticks(range(min_clusters, max_clusters + 1, 2))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q9_DwLW1KTWF"
   },
   "outputs": [],
   "source": [
    "# Plot Silhouette scores\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range_of_clusters, silhouette_scores, marker='o')\n",
    "plt.title('Silhouette Score vs. Number of Clusters')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.xticks(range(min_clusters, max_clusters + 1, 2))\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot WCSS values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range_of_clusters, wcss_values, marker='o')\n",
    "plt.title('Within-Cluster Sum of Squares (WCSS) vs. Number of Clusters')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('WCSS')\n",
    "plt.xticks(range(min_clusters, max_clusters + 1, 2))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### CLuster using specific K-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6j3vpIznKTWG"
   },
   "outputs": [],
   "source": [
    "num_clusters = 50\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42,n_init=50,max_iter=1000)\n",
    "kmeans.fit(document_embeddings)\n",
    "\n",
    "cluster_labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Plotting in 2D space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bcq12Ka5KTWG"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Perform PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "document_vectors_2d = pca.fit_transform(document_embeddings)\n",
    "\n",
    "# Plot the scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "for cluster in range(num_clusters):\n",
    "    plt.scatter(document_vectors_2d[cluster_labels == cluster, 0],\n",
    "                document_vectors_2d[cluster_labels == cluster, 1],\n",
    "                label=f'Cluster {cluster+1}')\n",
    "plt.title('K-means Clustering Results')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Reading the clusters and corresponding documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UIUbUO-gKTWG"
   },
   "outputs": [],
   "source": [
    "for i in range (50):\n",
    "  cluster_to_inspect = i\n",
    "  documents_in_cluster = [doc for doc, label in zip(docs, cluster_labels) if label == cluster_to_inspect]\n",
    "\n",
    "  for i, doc in enumerate(documents_in_cluster):\n",
    "      print(f\"Document {i+1} in Cluster {cluster_to_inspect+1}:\")\n",
    "      print(doc)\n",
    "      print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duW_mhOMLcmh"
   },
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Tf-IDF weighted average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OGf35Ebj1Bp"
   },
   "source": [
    "#### Without special parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gHd3PSgTKTWG"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "import concurrent.futures\n",
    "\n",
    "# Range of clusters to try\n",
    "min_clusters = 2\n",
    "max_clusters = 1800\n",
    "range_clusters = range(min_clusters, max_clusters + 1)\n",
    "\n",
    "# Function to compute WCSS and Silhouette Score for a given number of clusters\n",
    "def compute_metrics(num_clusters):\n",
    "    print(\"Clustering: \", num_clusters, '/', max_clusters)\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    kmeans.fit(document_vectors)\n",
    "    wcss = kmeans.inertia_\n",
    "    silhouette_avg = silhouette_score(document_vectors, kmeans.labels_)\n",
    "    return num_clusters, wcss, silhouette_avg\n",
    "\n",
    "# Lists to store evaluation metrics\n",
    "wcss_values = []\n",
    "silhouette_scores = []\n",
    "\n",
    "# Multithreaded computation\n",
    "max_workers = 4  # Set the maximum number of workers\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    # Submit tasks\n",
    "    futures = [executor.submit(compute_metrics, num_clusters) for num_clusters in range_clusters]\n",
    "    # Retrieve results\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        num_clusters, wcss, silhouette_avg = future.result()\n",
    "        wcss_values.append(wcss)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "# Plot the Elbow Method and Silhouette Scores\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range_clusters, wcss_values, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\n",
    "plt.title('Elbow Method')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range_clusters, silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Analysis')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Cluster using specific K-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "num_clusters = 250\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(document_vectors)\n",
    "\n",
    "cluster_labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Plotting in 2D space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "# Perform PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "document_vectors_2d = pca.fit_transform(document_vectors)\n",
    "\n",
    "# Plot the scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "for cluster in range(num_clusters):\n",
    "    plt.scatter(document_vectors_2d[cluster_labels == cluster, 0],\n",
    "                document_vectors_2d[cluster_labels == cluster, 1],\n",
    "                label=f'Cluster {cluster+1}')\n",
    "plt.title('K-means Clustering Results')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Read the clusters and corresponding documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range (250):\n",
    "  cluster_to_inspect = i\n",
    "  documents_in_cluster = [doc for doc, label in zip(docs, cluster_labels) if label == cluster_to_inspect]\n",
    "\n",
    "  # Print or display the documents\n",
    "  for i, doc in enumerate(documents_in_cluster):\n",
    "      print(f\"Document {i+1} in Cluster {cluster_to_inspect+1}:\")\n",
    "      print(doc)\n",
    "      print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Kmeans Parameters: n_int=10, max_iter=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iva3eqg0KTWH"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "\n",
    "# Range of clusters to try\n",
    "min_clusters = 2\n",
    "max_clusters = 250\n",
    "range_clusters = range(min_clusters, max_clusters + 1)\n",
    "\n",
    "# Function to compute WCSS, Silhouette Score, and Gap Statistic for a given number of clusters\n",
    "def compute_metrics(num_clusters):\n",
    "    print(\"Clustering: \", num_clusters, '/', max_clusters)\n",
    "    kmeans = KMeans(n_clusters=num_clusters, n_init=10, max_iter=20,random_state=42)\n",
    "    kmeans.fit(document_vectors)\n",
    "    wcss = kmeans.inertia_\n",
    "    silhouette_avg = silhouette_score(document_vectors, kmeans.labels_)\n",
    "    print(\"clustered: \",num_clusters)\n",
    "    return num_clusters, wcss, silhouette_avg\n",
    "\n",
    "\n",
    "# Lists to store evaluation metrics\n",
    "wcss_values = []\n",
    "silhouette_scores = []\n",
    "\n",
    "# Convert document_vectors to numpy array\n",
    "#document_vectors = np.array(document_vectors)\n",
    "\n",
    "# Multithreaded computation\n",
    "max_workers = 8  # Set the maximum number of workers\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    # Submit tasks\n",
    "    futures = [executor.submit(compute_metrics, num_clusters) for num_clusters in range_clusters]\n",
    "    # Retrieve results\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        num_clusters, wcss, silhouette_avg = future.result()\n",
    "        wcss_values.append(wcss)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "\n",
    "# Plot the Elbow Method, Silhouette Scores, and Gap Statistic\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(range_clusters, wcss_values, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\n",
    "plt.title('Elbow Method')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(range_clusters, silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Analysis')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Cluster using specific K-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CKaOxn7RKTWH"
   },
   "outputs": [],
   "source": [
    "num_clusters = 250\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(document_vectors)\n",
    "\n",
    "cluster_labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Plotting in 2D space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8LAip4fuKTWI"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Perform PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "document_vectors_2d = pca.fit_transform(document_vectors)\n",
    "\n",
    "# Plot the scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "for cluster in range(num_clusters):\n",
    "    plt.scatter(document_vectors_2d[cluster_labels == cluster, 0],\n",
    "                document_vectors_2d[cluster_labels == cluster, 1],\n",
    "                label=f'Cluster {cluster+1}')\n",
    "plt.title('K-means Clustering Results')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Read the clusters and corresponding documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YvXOnWapKTWI"
   },
   "outputs": [],
   "source": [
    "for i in range (250):\n",
    "  cluster_to_inspect = i\n",
    "  documents_in_cluster = [doc for doc, label in zip(docs, cluster_labels) if label == cluster_to_inspect]\n",
    "\n",
    "  # Print or display the documents\n",
    "  for i, doc in enumerate(documents_in_cluster):\n",
    "      print(f\"Document {i+1} in Cluster {cluster_to_inspect+1}:\")\n",
    "      print(doc)\n",
    "      print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26hTHR3CZW_Y"
   },
   "source": [
    "## Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Iy9m7s4KTWJ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "import concurrent.futures\n",
    "\n",
    "# Range of clusters to try\n",
    "min_clusters = 2\n",
    "max_clusters = 1800\n",
    "range_clusters = range(min_clusters, max_clusters + 1)\n",
    "\n",
    "# Function to compute WCSS and Silhouette Score for a given number of clusters\n",
    "def compute_metrics(num_clusters):\n",
    "    print(\"Clustering: \", num_clusters, '/', max_clusters)\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    kmeans.fit(sum)\n",
    "    wcss = kmeans.inertia_\n",
    "    silhouette_avg = silhouette_score(document_vectors, kmeans.labels_)\n",
    "    return num_clusters, wcss, silhouette_avg\n",
    "\n",
    "# Lists to store evaluation metrics\n",
    "wcss_values = []\n",
    "silhouette_scores = []\n",
    "\n",
    "# Multithreaded computation\n",
    "max_workers = 4  # Set the maximum number of workers\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    # Submit tasks\n",
    "    futures = [executor.submit(compute_metrics, num_clusters) for num_clusters in range_clusters]\n",
    "    # Retrieve results\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        num_clusters, wcss, silhouette_avg = future.result()\n",
    "        wcss_values.append(wcss)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "# Plot the Elbow Method and Silhouette Scores\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range_clusters, wcss_values, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\n",
    "plt.title('Elbow Method')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range_clusters, silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Analysis')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUjR-6wGZgM9"
   },
   "source": [
    "## Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XR6laufXKTWJ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "import concurrent.futures\n",
    "\n",
    "# Range of clusters to try\n",
    "min_clusters = 2\n",
    "max_clusters = 1800\n",
    "range_clusters = range(min_clusters, max_clusters + 1)\n",
    "\n",
    "# Function to compute WCSS and Silhouette Score for a given number of clusters\n",
    "def compute_metrics(num_clusters):\n",
    "    print(\"Clustering: \", num_clusters, '/', max_clusters)\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    kmeans.fit(average)\n",
    "    wcss = kmeans.inertia_\n",
    "    silhouette_avg = silhouette_score(document_vectors, kmeans.labels_)\n",
    "    return num_clusters, wcss, silhouette_avg\n",
    "\n",
    "# Lists to store evaluation metrics\n",
    "wcss_values = []\n",
    "silhouette_scores = []\n",
    "\n",
    "# Multithreaded computation\n",
    "max_workers = 8  # Set the maximum number of workers\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    # Submit tasks\n",
    "    futures = [executor.submit(compute_metrics, num_clusters) for num_clusters in range_clusters]\n",
    "    # Retrieve results\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        num_clusters, wcss, silhouette_avg = future.result()\n",
    "        wcss_values.append(wcss)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "# Plot the Elbow Method and Silhouette Scores\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range_clusters, wcss_values, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\n",
    "plt.title('Elbow Method')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range_clusters, silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Analysis')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4WVOG5wIKTWJ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "\n",
    "# Range of clusters to try\n",
    "min_clusters = 50\n",
    "max_clusters = 100\n",
    "range_clusters = range(min_clusters, max_clusters + 1)\n",
    "\n",
    "# Function to compute WCSS, Silhouette Score, and Gap Statistic for a given number of clusters\n",
    "def compute_metrics(num_clusters):\n",
    "    print(\"Clustering: \", num_clusters, '/', max_clusters)\n",
    "    kmeans = KMeans(n_clusters=num_clusters, n_init=10, max_iter=20,random_state=42)\n",
    "    kmeans.fit(document_vectors)\n",
    "    wcss = kmeans.inertia_\n",
    "    silhouette_avg = silhouette_score(document_vectors, kmeans.labels_)\n",
    "    print(\"clustered: \",num_clusters)\n",
    "    return num_clusters, wcss, silhouette_avg\n",
    "\n",
    "\n",
    "# Lists to store evaluation metrics\n",
    "wcss_values = []\n",
    "silhouette_scores = []\n",
    "\n",
    "# Convert document_vectors to numpy array\n",
    "#document_vectors = np.array(document_vectors)\n",
    "\n",
    "# Multithreaded computation\n",
    "max_workers = 8  # Set the maximum number of workers\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    # Submit tasks\n",
    "    futures = [executor.submit(compute_metrics, num_clusters) for num_clusters in range_clusters]\n",
    "    # Retrieve results\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        num_clusters, wcss, silhouette_avg = future.result()\n",
    "        wcss_values.append(wcss)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "\n",
    "# Plot the Elbow Method, Silhouette Scores, and Gap Statistic\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(range_clusters, wcss_values, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\n",
    "plt.title('Elbow Method')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(range_clusters, silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Analysis')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i8XALpq0KTWO"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "best_k =  250\n",
    "best_kmeans = KMeans(n_clusters=best_k, n_init=50, max_iter=300)\n",
    "best_kmeans.fit(document_vectors)\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UpRd1t6_KTWO"
   },
   "outputs": [],
   "source": [
    "# Reduce dimensionality with t-SNE\n",
    "tsne = TSNE(n_components=3, random_state=42)\n",
    "X_tsne = tsne.fit_transform(document_vectors)\n",
    "\n",
    "# Plot the result in 3D\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for cluster in range(best_k):\n",
    "    ax.scatter(X_tsne[best_kmeans.labels_ == cluster, 0],\n",
    "               X_tsne[best_kmeans.labels_ == cluster, 1],\n",
    "               X_tsne[best_kmeans.labels_ == cluster, 2],\n",
    "               label=f'Cluster {cluster}')\n",
    "ax.set_title(f'KMeans Clustering Result with {best_k} Clusters (t-SNE)')\n",
    "ax.set_xlabel('t-SNE Component 1')\n",
    "ax.set_ylabel('t-SNE Component 2')\n",
    "ax.set_zlabel('t-SNE Component 3')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "import concurrent.futures\n",
    "\n",
    "# Range of clusters to try\n",
    "min_clusters = 2\n",
    "max_clusters = 1800\n",
    "range_clusters = range(min_clusters, max_clusters + 1)\n",
    "\n",
    "# Function to compute WCSS and Silhouette Score for a given number of clusters\n",
    "def compute_metrics(num_clusters):\n",
    "    print(\"Clustering: \", num_clusters, '/', max_clusters)\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    kmeans.fit(tfidf_matrix)\n",
    "    wcss = kmeans.inertia_\n",
    "    silhouette_avg = silhouette_score(tfidf_matrix, kmeans.labels_)\n",
    "    return num_clusters, wcss, silhouette_avg\n",
    "\n",
    "# Lists to store evaluation metrics\n",
    "wcss_values = []\n",
    "silhouette_scores = []\n",
    "\n",
    "# Multithreaded computation\n",
    "max_workers = 2  # Set the maximum number of workers\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    # Submit tasks\n",
    "    futures = [executor.submit(compute_metrics, num_clusters) for num_clusters in range_clusters]\n",
    "    # Retrieve results\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        num_clusters, wcss, silhouette_avg = future.result()\n",
    "        wcss_values.append(wcss)\n",
    "        silhouette_scores.append(silhouette_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plot the Elbow Method and Silhouette Scores\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range_clusters, wcss_values, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\n",
    "plt.title('Elbow Method')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range_clusters, silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Analysis')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3uK7lBNKTWO"
   },
   "source": [
    "# Using LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "num_topics = 10\n",
    "\n",
    "# Initialize LDA model\n",
    "lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "\n",
    "# Fit LDA model to TF-IDF matrix\n",
    "lda_model.fit(tfidf_matrix)\n",
    "\n",
    "# Print topics\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        print()\n",
    "\n",
    "# Get feature names from TfidfVectorizer\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print topics\n",
    "print_top_words(lda_model, feature_names, n_top_words=10)\n",
    "\n",
    "# Save topics to a text file\n",
    "with open('topics.txt', 'w') as f:\n",
    "    for topic_idx, topic in enumerate(lda_model.components_):\n",
    "        f.write(f\"Topic {topic_idx}:\\n\")\n",
    "        f.write(\" \".join([feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "\n",
    "# Check the length of the mails list\n",
    "print(\"Length of mails:\", len(mails))\n",
    "\n",
    "num_topics = 10\n",
    "\n",
    "# Initialize LDA model\n",
    "lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "\n",
    "# Fit LDA model to TF-IDF matrix\n",
    "lda_model.fit(tfidf_matrix)\n",
    "\n",
    "# Check the length of lda_model.components_\n",
    "print(\"Length of lda_model.components_:\", len(lda_model.components_))\n",
    "\n",
    "# Get document-topic distribution\n",
    "doc_topic_distribution = lda_model.transform(tfidf_matrix)\n",
    "\n",
    "# Check the shape of doc_topic_distribution\n",
    "print(\"Shape of doc_topic_distribution:\", doc_topic_distribution.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('document_topic_assignment.txt', 'w') as f:\n",
    "    for i, doc_topics in enumerate(doc_topic_distribution):\n",
    "        print(\"Document index:\", i)\n",
    "        sorted_topics = np.argsort(doc_topics)[::-1]  # Sort topics by probability in descending order\n",
    "        for j, topic_idx in enumerate(sorted_topics):\n",
    "            print(\"Topic index:\", topic_idx)\n",
    "            print(\"Length of mails:\", len(mails))\n",
    "            print(\"Length of lda_model.components_:\", len(lda_model.components_))\n",
    "            f.write(f\"Topic {topic_idx}: Probability={doc_topics[topic_idx]}\\n\")\n",
    "            # Include top N documents most associated with the current topic\n",
    "            if j == 0:  # Adjust N as needed\n",
    "                f.write(\"Associated Documents:\\n\")\n",
    "                top_docs_indices = np.argsort(lda_model.components_[topic_idx])[::-1][:5]  # Top 5 associated documents\n",
    "                print(\"Top document indices:\", top_docs_indices)\n",
    "                print(\"Length of top_docs_indices:\", len(top_docs_indices))\n",
    "                if len(top_docs_indices) > 0:  # Check if top_docs_indices is not empty\n",
    "                    for doc_idx in top_docs_indices:\n",
    "                        if doc_idx < len(mails):  # Check if doc_idx is within valid range\n",
    "                            print(\"Document index within top_docs_indices loop:\", doc_idx)\n",
    "                            f.write(f\"Document {doc_idx}: {mails[doc_idx]}\\n\")\n",
    "                        else:\n",
    "                            print(f\"Ignore document index {doc_idx} as it is out of range\")\n",
    "                else:\n",
    "                    print(f\"No associated documents found for topic {topic_idx}\")\n",
    "        f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define pipeline with TF-IDF, LDA, and K-means\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('lda', LatentDirichletAllocation(n_components=10, random_state=42)),\n",
    "    ('norm', Normalizer()),  # Normalize LDA topic distributions\n",
    "    ('kmeans', KMeans(n_clusters=5, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit pipeline to data\n",
    "pipeline.fit(mails)\n",
    "\n",
    "# Get document-topic distributions from LDA\n",
    "lda_topics = pipeline.named_steps['lda'].transform(pipeline.named_steps['tfidf'].transform(mails))\n",
    "\n",
    "# Get cluster assignments from K-means\n",
    "kmeans_clusters = pipeline.named_steps['kmeans'].labels_\n",
    "\n",
    "# Output results\n",
    "for i in range(len(mails)):\n",
    "    print(f\"Document {mails[i]}:\")\n",
    "    print(f\"  LDA Topics: {lda_topics[i]}\")\n",
    "    print(f\"  K-means Cluster: {kmeans_clusters[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Define range of cluster values to evaluate\n",
    "cluster_values = range(2, 50)\n",
    "wcss_scores = []\n",
    "silhouette_scores = []\n",
    "\n",
    "for n_clusters in cluster_values:\n",
    "    # Update K-means cluster value\n",
    "    pipeline.named_steps['kmeans'].set_params(n_clusters=n_clusters)\n",
    "\n",
    "    # Fit pipeline to data\n",
    "    pipeline.fit(mails)\n",
    "\n",
    "    # Get cluster assignments from K-means\n",
    "    kmeans_clusters = pipeline.named_steps['kmeans'].labels_\n",
    "\n",
    "    # Calculate Within-Cluster Sum of Squares (WCSS)\n",
    "    wcss = pipeline.named_steps['kmeans'].inertia_\n",
    "    wcss_scores.append(wcss)\n",
    "\n",
    "    # Calculate silhouette score\n",
    "    silhouette_avg = silhouette_score(lda_topics, kmeans_clusters)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "# Plot WCSS scores\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(cluster_values, wcss_scores, marker='o', linestyle='-')\n",
    "plt.title('Within-Cluster Sum of Squares (WCSS) vs. Number of Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.xticks(cluster_values)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot silhouette scores\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(cluster_values, silhouette_scores, marker='o', linestyle='-')\n",
    "plt.title('Silhouette Score vs. Number of Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.xticks(cluster_values)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define range of topic values and cluster values to evaluate\n",
    "topic_values = range(2, 25)\n",
    "cluster_values = range(2, 35)\n",
    "\n",
    "for n_topics in topic_values:\n",
    "    print(f\"Analyzing topics: {n_topics}\")\n",
    "    wcss_scores = []\n",
    "    silhouette_scores = []\n",
    "\n",
    "    for n_clusters in cluster_values:\n",
    "        print(f\"  Analyzing clusters: {n_clusters}\")\n",
    "        # Define pipeline with TF-IDF, LDA, and K-means\n",
    "        pipeline = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('lda', LatentDirichletAllocation(n_components=n_topics, random_state=42)),\n",
    "            ('norm', Normalizer()),  # Normalize LDA topic distributions\n",
    "            ('kmeans', KMeans(n_clusters=n_clusters, random_state=42))\n",
    "        ])\n",
    "\n",
    "        # Fit pipeline to data\n",
    "        pipeline.fit(mails)\n",
    "\n",
    "        # Get document-topic distributions from LDA\n",
    "        lda_topics = pipeline.named_steps['lda'].transform(pipeline.named_steps['tfidf'].transform(mails))\n",
    "\n",
    "        # Get cluster assignments from K-means\n",
    "        kmeans_clusters = pipeline.named_steps['kmeans'].labels_\n",
    "\n",
    "        # Calculate Within-Cluster Sum of Squares (WCSS)\n",
    "        wcss = pipeline.named_steps['kmeans'].inertia_\n",
    "        wcss_scores.append(wcss)\n",
    "\n",
    "        # Calculate silhouette score\n",
    "        silhouette_avg = silhouette_score(lda_topics, kmeans_clusters)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "    # Plot WCSS scores for each cluster value\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(cluster_values, wcss_scores, marker='o', linestyle='-')\n",
    "    plt.title(f'WCSS vs. Number of Clusters (Topics={n_topics})')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('WCSS')\n",
    "    plt.xticks(cluster_values)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot silhouette scores for each cluster value\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(cluster_values, silhouette_scores, marker='o', linestyle='-')\n",
    "    plt.title(f'Silhouette Score vs. Number of Clusters (Topics={n_topics})')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.xticks(cluster_values)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define range of topic values and cluster values to evaluate\n",
    "topic_values = range(2, 30)\n",
    "cluster_values = range(2, 40)\n",
    "\n",
    "for n_topics in topic_values:\n",
    "    print(f\"Analyzing topics: {n_topics}\")\n",
    "    wcss_scores = []\n",
    "    silhouette_scores = []\n",
    "\n",
    "    for n_clusters in cluster_values:\n",
    "        print(f\"  Analyzing clusters: {n_clusters}\")\n",
    "        # Define pipeline with TF-IDF, LDA, and K-means\n",
    "        pipeline = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('lda', LatentDirichletAllocation(n_components=n_topics, random_state=42)),\n",
    "            ('norm', Normalizer()),  # Normalize LDA topic distributions\n",
    "            ('kmeans', KMeans(n_clusters=n_clusters, random_state=42, n_init=30))\n",
    "        ])\n",
    "\n",
    "        # Fit pipeline to data\n",
    "        pipeline.fit(mails)\n",
    "\n",
    "        # Get document-topic distributions from LDA\n",
    "        lda_topics = pipeline.named_steps['lda'].transform(pipeline.named_steps['tfidf'].transform(mails))\n",
    "\n",
    "        # Get cluster assignments from K-means\n",
    "        kmeans_clusters = pipeline.named_steps['kmeans'].labels_\n",
    "\n",
    "        # Calculate Within-Cluster Sum of Squares (WCSS)\n",
    "        wcss = pipeline.named_steps['kmeans'].inertia_\n",
    "        wcss_scores.append(wcss)\n",
    "\n",
    "        # Calculate silhouette score\n",
    "        silhouette_avg = silhouette_score(lda_topics, kmeans_clusters)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "    # Plot WCSS scores for each cluster value\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(cluster_values, wcss_scores, marker='o', linestyle='-')\n",
    "    plt.title(f'WCSS vs. Number of Clusters (Topics={n_topics})')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('WCSS')\n",
    "    plt.xticks(cluster_values)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot silhouette scores for each cluster value\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(cluster_values, silhouette_scores, marker='o', linestyle='-')\n",
    "    plt.title(f'Silhouette Score vs. Number of Clusters (Topics={n_topics})')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.xticks(cluster_values)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define range of topic values and cluster values to evaluate\n",
    "topic_values = range(2, 25)\n",
    "cluster_values = range(2, 20)\n",
    "\n",
    "for n_topics in topic_values:\n",
    "    print(f\"Analyzing topics: {n_topics}\")\n",
    "    wcss_scores = []\n",
    "    silhouette_scores = []\n",
    "\n",
    "    for n_clusters in cluster_values:\n",
    "        print(f\"  Analyzing clusters: {n_clusters}\")\n",
    "        # Define pipeline with BoW, LDA, and K-means\n",
    "        pipeline = Pipeline([\n",
    "            ('bow', CountVectorizer()),  # Changed to CountVectorizer\n",
    "            ('lda', LatentDirichletAllocation(n_components=n_topics, random_state=42)),\n",
    "            ('norm', Normalizer()),  # Normalize LDA topic distributions\n",
    "            ('kmeans', KMeans(n_clusters=n_clusters, random_state=42))\n",
    "        ])\n",
    "\n",
    "        # Fit pipeline to data\n",
    "        pipeline.fit(mails)\n",
    "\n",
    "        # Get document-topic distributions from LDA\n",
    "        lda_topics = pipeline.named_steps['lda'].transform(pipeline.named_steps['bow'].transform(mails))\n",
    "\n",
    "        # Get cluster assignments from K-means\n",
    "        kmeans_clusters = pipeline.named_steps['kmeans'].labels_\n",
    "\n",
    "        # Calculate Within-Cluster Sum of Squares (WCSS)\n",
    "        wcss = pipeline.named_steps['kmeans'].inertia_\n",
    "        wcss_scores.append(wcss)\n",
    "\n",
    "        # Calculate silhouette score\n",
    "        silhouette_avg = silhouette_score(lda_topics, kmeans_clusters)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "    # Plot WCSS scores for each cluster value\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(cluster_values, wcss_scores, marker='o', linestyle='-')\n",
    "    plt.title(f'WCSS vs. Number of Clusters (Topics={n_topics})')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('WCSS')\n",
    "    plt.xticks(cluster_values)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot silhouette scores for each cluster value\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(cluster_values, silhouette_scores, marker='o', linestyle='-')\n",
    "    plt.title(f'Silhouette Score vs. Number of Clusters (Topics={n_topics})')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.xticks(cluster_values)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define range of topic values to evaluate\n",
    "topic_values = range(2, 20)\n",
    "perplexity_scores = []\n",
    "\n",
    "for n_topics in topic_values:\n",
    "    print(f\"Analyzing topics: {n_topics}\")\n",
    "\n",
    "    # Define pipeline with BoW and LDA\n",
    "    pipeline = Pipeline([\n",
    "        ('bow', CountVectorizer()),  # Changed to CountVectorizer\n",
    "        ('lda', LatentDirichletAllocation(n_components=n_topics, random_state=42))\n",
    "    ])\n",
    "\n",
    "    # Fit pipeline to data\n",
    "    pipeline.fit(mails)\n",
    "\n",
    "    # Calculate perplexity\n",
    "    perplexity = pipeline.named_steps['lda'].perplexity(pipeline.named_steps['bow'].transform(mails))\n",
    "    perplexity_scores.append(perplexity)\n",
    "\n",
    "# Plot perplexity scores for each topic value\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(topic_values, perplexity_scores, marker='o', linestyle='-')\n",
    "plt.title('Perplexity vs. Number of Topics')\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define range of topic values to evaluate\n",
    "topic_values = range(2, 25)\n",
    "perplexity_scores = []\n",
    "coherence_scores = []\n",
    "diversity_scores = []\n",
    "\n",
    "# Tokenize each email string into a list of words\n",
    "tokenized_mails = [mail.split() for mail in mails]\n",
    "\n",
    "# Create dictionary from the tokenized emails\n",
    "dictionary = Dictionary(tokenized_mails)\n",
    "\n",
    "for n_topics in topic_values:\n",
    "    print(f\"Analyzing topics: {n_topics}\")\n",
    "\n",
    "    # Create bag-of-words corpus\n",
    "    corpus = [dictionary.doc2bow(text) for text in tokenized_mails]  # Use tokenized_mails instead of mails\n",
    "\n",
    "    # Fit LDA model\n",
    "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=n_topics, random_state=42)\n",
    "\n",
    "    # Calculate perplexity\n",
    "    perplexity = lda_model.log_perplexity(corpus)\n",
    "    perplexity_scores.append(np.exp2(-perplexity))\n",
    "\n",
    "    # Calculate coherence score\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=tokenized_mails, dictionary=dictionary, coherence='c_v')  # Use tokenized_mails instead of mails\n",
    "    coherence = coherence_model_lda.get_coherence()\n",
    "    coherence_scores.append(coherence)\n",
    "\n",
    "    # Calculate topic diversity\n",
    "    topics = lda_model.get_topics()\n",
    "    diversity = np.mean([len(set(topic)) / len(topic) for topic in topics])\n",
    "    diversity_scores.append(diversity)\n",
    "\n",
    "# Plot all metrics\n",
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Number of Topics')\n",
    "ax1.set_ylabel('Perplexity', color=color)\n",
    "ax1.plot(topic_values, perplexity_scores, marker='o', linestyle='-', color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Coherence', color=color)\n",
    "ax2.plot(topic_values, coherence_scores, marker='o', linestyle='-', color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax3 = ax1.twinx()\n",
    "color = 'tab:green'\n",
    "ax3.spines['right'].set_position(('outward', 60))  # move the spine\n",
    "ax3.set_ylabel('Diversity', color=color)\n",
    "ax3.plot(topic_values, diversity_scores, marker='o', linestyle='-', color=color)\n",
    "ax3.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.title('Evaluation Metrics vs. Number of Topics')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot all metrics\n",
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Number of Topics')\n",
    "ax1.set_ylabel('Perplexity', color=color)\n",
    "ax1.plot(topic_values, perplexity_scores, marker='o', linestyle='-', color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Coherence', color=color)\n",
    "ax2.plot(topic_values, coherence_scores, marker='o', linestyle='-', color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax3 = ax1.twinx()\n",
    "color = 'tab:green'\n",
    "ax3.spines['right'].set_position(('outward', 60))  # move the spine\n",
    "ax3.set_ylabel('Diversity', color=color)\n",
    "ax3.plot(topic_values, diversity_scores, marker='o', linestyle='-', color=color)\n",
    "ax3.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.title('Evaluation Metrics vs. Number of Topics')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "from gensim import corpora\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define range of topic values to evaluate\n",
    "topic_values = range(2, 25)\n",
    "perplexity_scores = []\n",
    "coherence_scores = []\n",
    "diversity_scores = []\n",
    "\n",
    "# Tokenize each email string into a list of words\n",
    "tokenized_mails = [mail.split() for mail in mails]\n",
    "\n",
    "# Create dictionary from the tokenized emails\n",
    "dictionary = corpora.Dictionary(tokenized_mails)\n",
    "\n",
    "# Create TF-IDF representation\n",
    "tfidf = TfidfModel(dictionary=dictionary)\n",
    "corpus_tfidf = tfidf[[dictionary.doc2bow(text) for text in tokenized_mails]]\n",
    "\n",
    "for n_topics in topic_values:\n",
    "    print(f\"Analyzing topics: {n_topics}\")\n",
    "\n",
    "    # Fit LDA model\n",
    "    lda_model = LdaModel(corpus=corpus_tfidf, id2word=dictionary, num_topics=n_topics, random_state=42)\n",
    "\n",
    "    # Calculate perplexity\n",
    "    perplexity = lda_model.log_perplexity(corpus_tfidf)\n",
    "    perplexity_scores.append(np.exp2(-perplexity))\n",
    "\n",
    "    # Calculate coherence score\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=tokenized_mails, dictionary=dictionary, coherence='c_v')\n",
    "    coherence = coherence_model_lda.get_coherence()\n",
    "    coherence_scores.append(coherence)\n",
    "\n",
    "    # Calculate topic diversity\n",
    "    topics = lda_model.get_topics()\n",
    "    diversity = np.mean([len(set(topic)) / len(topic) for topic in topics])\n",
    "    diversity_scores.append(diversity)\n",
    "\n",
    "# Plot all metrics\n",
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Number of Topics')\n",
    "ax1.set_ylabel('Perplexity', color=color)\n",
    "ax1.plot(topic_values, perplexity_scores, marker='o', linestyle='-', color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Coherence', color=color)\n",
    "ax2.plot(topic_values, coherence_scores, marker='o', linestyle='-', color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax3 = ax1.twinx()\n",
    "color = 'tab:green'\n",
    "ax3.spines['right'].set_position(('outward', 60))  # move the spine\n",
    "ax3.set_ylabel('Diversity', color=color)\n",
    "ax3.plot(topic_values, diversity_scores, marker='o', linestyle='-', color=color)\n",
    "ax3.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.title('Evaluation Metrics vs. Number of Topics')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Using SVM with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "mails=[]\n",
    "file_location=[]\n",
    "for file in os.listdir('Corpus'):\n",
    "    if file!='The_whole_corpus.txt':\n",
    "        with open(f\"Corpus/{(file)}\",'r',encoding='utf-8') as f:\n",
    "            mails.append(f.read())\n",
    "            file_location.append(f\"Corpus/{(file)}\")\n",
    "            #print(\"added: \",file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Tokenize each email string into a list of words\n",
    "tokenized_mails = [mail.split() for mail in mails]\n",
    "\n",
    "# Create dictionary from the tokenized emails\n",
    "dictionary = Dictionary(tokenized_mails)\n",
    "\n",
    "# Create bag-of-words corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_mails]\n",
    "\n",
    "# Define the number of topics\n",
    "num_topics = 9\n",
    "\n",
    "# Train LDA model\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=42)\n",
    "\n",
    "# Print topics and their top words\n",
    "for topic_id, topic_words in lda_model.print_topics():\n",
    "    print(f\"Topic {topic_id + 1}: {topic_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Preprocess each email in 'mails' list\n",
    "preprocessed_mails = [mail.split() for mail in mails]\n",
    "\n",
    "# Create a dictionary representation of the emails\n",
    "dictionary = corpora.Dictionary(preprocessed_mails)\n",
    "\n",
    "# Create a document-term matrix\n",
    "corpus = [dictionary.doc2bow(mail) for mail in preprocessed_mails]\n",
    "\n",
    "# Train the LDA model\n",
    "lda_model = gensim.models.LdaModel(corpus, num_topics=9, id2word=dictionary, passes=15)\n",
    "\n",
    "# Get the topic distributions for each document\n",
    "topic_distributions = [lda_model.get_document_topics(doc) for doc in corpus]\n",
    "\n",
    "# Extract topic proportions for each document\n",
    "topic_proportions = np.zeros((len(topic_distributions), lda_model.num_topics))\n",
    "for i, doc_topics in enumerate(topic_distributions):\n",
    "    for topic, proportion in doc_topics:\n",
    "        topic_proportions[i, topic] = proportion\n",
    "\n",
    "# Perform dimensionality reduction using t-SNE\n",
    "tsne_model = TSNE(n_components=2, random_state=42)\n",
    "tsne_topics = tsne_model.fit_transform(topic_proportions)\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "df_tsne = pd.DataFrame(tsne_topics, columns=['Dimension 1', 'Dimension 2'])\n",
    "\n",
    "# Plot the 2D visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df_tsne, x='Dimension 1', y='Dimension 2')\n",
    "plt.title('2D Visualization of LDA Topics')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Preprocess each email in 'mails' list\n",
    "preprocessed_mails = [mail.split() for mail in mails]\n",
    "\n",
    "# Create a dictionary representation of the emails\n",
    "dictionary = corpora.Dictionary(preprocessed_mails)\n",
    "\n",
    "# Create a document-term matrix\n",
    "corpus = [dictionary.doc2bow(mail) for mail in preprocessed_mails]\n",
    "\n",
    "# Train the LDA model\n",
    "lda_model = gensim.models.LdaModel(corpus, num_topics=9, id2word=dictionary, passes=15)\n",
    "\n",
    "# Get the topic distributions for each document\n",
    "topic_distributions = [lda_model.get_document_topics(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract topic proportions for each document\n",
    "topic_proportions = np.zeros((len(topic_distributions), lda_model.num_topics))\n",
    "for i, doc_topics in enumerate(topic_distributions):\n",
    "    for topic, proportion in doc_topics:\n",
    "        topic_proportions[i, topic] = proportion\n",
    "\n",
    "# Print the filled topic proportions\n",
    "for i, topic_dist in enumerate(topic_proportions):\n",
    "    print(i, \":\", topic_dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Range of clusters to try\n",
    "min_clusters = 2\n",
    "max_clusters = 30\n",
    "cluster_range = range(min_clusters, max_clusters + 1)\n",
    "\n",
    "# Initialize lists to store silhouette scores and WCSS\n",
    "silhouette_scores = []\n",
    "wcss = []\n",
    "\n",
    "# Calculate silhouette score and WCSS for each cluster\n",
    "for n_clusters in cluster_range:\n",
    "    # Initialize KMeans with the current number of clusters\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=20)\n",
    "\n",
    "    # Fit KMeans to the topic proportions\n",
    "    kmeans.fit(topic_proportions)\n",
    "\n",
    "    # Get cluster labels and cluster centers\n",
    "    cluster_labels = kmeans.labels_\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "    # Calculate silhouette score\n",
    "    silhouette_scores.append(silhouette_score(topic_proportions, cluster_labels))\n",
    "\n",
    "    # Calculate WCSS\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plot silhouette score vs number of clusters\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(cluster_range, silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score vs Number of Clusters')\n",
    "\n",
    "# Plot WCSS vs number of clusters\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(cluster_range, wcss, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.title('WCSS vs Number of Clusters')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hardcoding the optimal number of clusters\n",
    "optimal_num_clusters = 8\n",
    "\n",
    "# Initialize KMeans with the optimal number of clusters\n",
    "kmeans = KMeans(n_clusters=optimal_num_clusters, random_state=42, n_init=40)\n",
    "\n",
    "# Fit KMeans to the topic proportions\n",
    "kmeans.fit(topic_proportions)\n",
    "\n",
    "# Get cluster labels\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Apply t-SNE for dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "topic_tsne = tsne.fit_transform(topic_proportions)\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "df_tsne = pd.DataFrame(topic_tsne, columns=['tsne1', 'tsne2'])\n",
    "df_tsne['Cluster'] = cluster_labels\n",
    "\n",
    "# Visualize clusters using seaborn\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(data=df_tsne, x='tsne1', y='tsne2', hue='Cluster', palette='viridis', alpha=0.8)\n",
    "plt.title('t-SNE Visualization of Clusters')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Apply t-SNE for dimensionality reduction with 3 components\n",
    "tsne = TSNE(n_components=3, random_state=42)\n",
    "topic_tsne_3d = tsne.fit_transform(topic_proportions)\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "df_tsne_3d = pd.DataFrame(topic_tsne_3d, columns=['tsne1', 'tsne2', 'tsne3'])\n",
    "df_tsne_3d['Cluster'] = cluster_labels\n",
    "\n",
    "# Visualize clusters using matplotlib 3D scatter plot\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot each cluster\n",
    "for cluster in range(optimal_num_clusters):\n",
    "    cluster_data = df_tsne_3d[df_tsne_3d['Cluster'] == cluster]\n",
    "    ax.scatter(cluster_data['tsne1'], cluster_data['tsne2'], cluster_data['tsne3'], label=f'Cluster {cluster}')\n",
    "\n",
    "ax.set_title('3D t-SNE Visualization of Clusters')\n",
    "ax.set_xlabel('t-SNE Component 1')\n",
    "ax.set_ylabel('t-SNE Component 2')\n",
    "ax.set_zlabel('t-SNE Component 3')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the average topic distributions for each cluster\n",
    "cluster_topic_distributions = []\n",
    "\n",
    "# Calculate the average topic distribution for each cluster\n",
    "for cluster in range(optimal_num_clusters):\n",
    "    # Filter documents belonging to the current cluster\n",
    "    cluster_indices = np.where(cluster_labels == cluster)[0]\n",
    "    cluster_topic_dist = np.mean(topic_proportions[cluster_indices], axis=0)\n",
    "    cluster_topic_distributions.append(cluster_topic_dist)\n",
    "\n",
    "# Convert the list to a numpy array for easier manipulation\n",
    "cluster_topic_distributions = np.array(cluster_topic_distributions)\n",
    "\n",
    "# Print the top topics for each cluster\n",
    "for cluster, topic_dist in enumerate(cluster_topic_distributions):\n",
    "    print(f\"Cluster {cluster}:\")\n",
    "    top_topics = np.argsort(topic_dist)[::-1][:3]  # Get indices of top 3 topics\n",
    "    for topic_idx in top_topics:\n",
    "        topic_words = lda_model.show_topic(topic_idx)  # Get top words for the topic\n",
    "        print(f\"    Topic {topic_idx}: {topic_words}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Function to load emails from files\n",
    "def load_emails_from_files(directory):\n",
    "    emails = []\n",
    "    file_locations = []\n",
    "    for file in os.listdir(directory):\n",
    "        if file != 'The_whole_corpus.txt':\n",
    "            with open(os.path.join(directory, file), 'r', encoding='utf-8') as f:\n",
    "                emails.append(f.read())\n",
    "                file_locations.append(os.path.join(directory, file))\n",
    "    return emails, file_locations\n",
    "\n",
    "# Load emails from files\n",
    "emails, file_locations = load_emails_from_files('Corpus')\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "\n",
    "# Fit and transform the data with TF-IDF Vectorizer\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(emails)\n",
    "\n",
    "# Get feature names\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the number of topics\n",
    "num_topics = 9\n",
    "\n",
    "# Initialize LDA model\n",
    "lda_model = LDA(n_components=num_topics, random_state=42)\n",
    "\n",
    "# Fit LDA model to the TF-IDF matrix\n",
    "lda_model.fit(tfidf_matrix)\n",
    "\n",
    "# Function to display topics and their top words\n",
    "def display_topics(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx + 1}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        print()\n",
    "\n",
    "# Print topics and their top words\n",
    "print(\"Top words per topic:\")\n",
    "display_topics(lda_model, tfidf_feature_names, 10)\n",
    "\n",
    "# Transform TF-IDF matrix into topic distributions\n",
    "topic_distributions = lda_model.transform(tfidf_matrix)\n",
    "\n",
    "# Perform dimensionality reduction using t-SNE\n",
    "tsne_model = TSNE(n_components=2, random_state=42)\n",
    "tsne_topics = tsne_model.fit_transform(topic_distributions)\n",
    "\n",
    "# Plot the 2D visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=tsne_topics[:, 0], y=tsne_topics[:, 1], alpha=0.7)\n",
    "plt.title('2D Visualization of LDA Topics (TF-IDF)')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.show()\n",
    "\n",
    "# Get topic proportions for each document\n",
    "topic_proportions = np.zeros((len(topic_distributions), num_topics))\n",
    "for i, doc_topic in enumerate(topic_distributions):\n",
    "    for j, proportion in enumerate(doc_topic):\n",
    "        topic_proportions[i, j] = proportion\n",
    "\n",
    "# Calculate silhouette scores and within-cluster sum of squares (WCSS) for different number of clusters\n",
    "min_clusters = 2\n",
    "max_clusters = 30\n",
    "cluster_range = range(min_clusters, max_clusters + 1)\n",
    "silhouette_scores = []\n",
    "wcss = []\n",
    "\n",
    "for n_clusters in cluster_range:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=20)\n",
    "    kmeans.fit(topic_proportions)\n",
    "    cluster_labels = kmeans.labels_\n",
    "    silhouette_scores.append(silhouette_score(topic_proportions, cluster_labels))\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plot silhouette scores vs number of clusters\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(cluster_range, silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score vs Number of Clusters')\n",
    "\n",
    "# Plot WCSS vs number of clusters\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(cluster_range, wcss, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.title('WCSS vs Number of Clusters')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Hardcoding the optimal number of clusters\n",
    "optimal_num_clusters = 8\n",
    "\n",
    "# Initialize KMeans with the optimal number of clusters\n",
    "kmeans = KMeans(n_clusters=optimal_num_clusters, random_state=42, n_init=40)\n",
    "kmeans.fit(topic_proportions)\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Apply t-SNE for dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "topic_tsne = tsne.fit_transform(topic_proportions)\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "df_tsne = pd.DataFrame(topic_tsne, columns=['tsne1', 'tsne2'])\n",
    "df_tsne['Cluster'] = cluster_labels\n",
    "\n",
    "# Visualize clusters using seaborn\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(data=df_tsne, x='tsne1', y='tsne2', hue='Cluster', palette='viridis', alpha=0.8)\n",
    "plt.title('t-SNE Visualization of Clusters (TF-IDF)')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Apply t-SNE for dimensionality reduction with 3 components\n",
    "tsne = TSNE(n_components=3, random_state=42)\n",
    "topic_tsne_3d = tsne.fit_transform(topic_proportions)\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "df_tsne_3d = pd.DataFrame(topic_tsne_3d, columns=['tsne1', 'tsne2', 'tsne3'])\n",
    "df_tsne_3d['Cluster'] = cluster_labels\n",
    "\n",
    "# Visualize clusters using matplotlib 3D scatter plot\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot each cluster\n",
    "for cluster in range(optimal_num_clusters):\n",
    "    cluster_data = df_tsne_3d[df_tsne_3d['Cluster'] == cluster]\n",
    "    ax.scatter(cluster_data['tsne1'], cluster_data['tsne2'], cluster_data['tsne3'], label=f'Cluster {cluster}')\n",
    "\n",
    "ax.set_title('3D t-SNE Visualization of Clusters (TF-IDF)')\n",
    "ax.set_xlabel('t-SNE Component 1')\n",
    "ax.set_ylabel('t-SNE Component 2')\n",
    "ax.set_zlabel('t-SNE Component 3')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## DBSCAN for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Initialize DBSCAN with epsilon and minimum samples\n",
    "epsilon = 0.3\n",
    "min_samples = 5\n",
    "dbscan = DBSCAN(eps=epsilon, min_samples=min_samples)\n",
    "\n",
    "# Fit DBSCAN to the topic proportions\n",
    "dbscan.fit(topic_proportions)\n",
    "\n",
    "# Get cluster labels\n",
    "cluster_labels = dbscan.labels_\n",
    "\n",
    "# Apply t-SNE for dimensionality reduction with 3 components\n",
    "tsne = TSNE(n_components=3, random_state=42)\n",
    "topic_tsne_3d = tsne.fit_transform(topic_proportions)\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "df_tsne_3d = pd.DataFrame(topic_tsne_3d, columns=['tsne1', 'tsne2', 'tsne3'])\n",
    "df_tsne_3d['Cluster'] = cluster_labels\n",
    "\n",
    "# Visualize clusters using matplotlib 3D scatter plot\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot each cluster\n",
    "unique_labels = np.unique(cluster_labels)\n",
    "for label in unique_labels:\n",
    "    if label == -1:\n",
    "        # Noise points are plotted as black\n",
    "        cluster_data = df_tsne_3d[df_tsne_3d['Cluster'] == label]\n",
    "        ax.scatter(cluster_data['tsne1'], cluster_data['tsne2'], cluster_data['tsne3'], c='black', label=f'Cluster {label}')\n",
    "    else:\n",
    "        cluster_data = df_tsne_3d[df_tsne_3d['Cluster'] == label]\n",
    "        ax.scatter(cluster_data['tsne1'], cluster_data['tsne2'], cluster_data['tsne3'], label=f'Cluster {label}')\n",
    "\n",
    "ax.set_title('3D t-SNE Visualization of Clusters with DBSCAN (TF-IDF)')\n",
    "ax.set_xlabel('t-SNE Component 1')\n",
    "ax.set_ylabel('t-SNE Component 2')\n",
    "ax.set_zlabel('t-SNE Component 3')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get unique cluster labels\n",
    "unique_labels = np.unique(cluster_labels)\n",
    "\n",
    "# Count the number of clusters (excluding noise points labeled as -1)\n",
    "num_clusters = len(unique_labels) - 1 if -1 in unique_labels else len(unique_labels)\n",
    "\n",
    "print(\"Number of clusters:\", num_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Range of epsilon values to try\n",
    "epsilon_values = [10]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, epsilon in enumerate(epsilon_values, 1):\n",
    "    print(\"doing\",i)\n",
    "    # Initialize DBSCAN with the current epsilon value\n",
    "    dbscan = DBSCAN(eps=epsilon, min_samples=5)\n",
    "    dbscan.fit(topic_proportions)\n",
    "\n",
    "    # Get cluster labels\n",
    "    cluster_labels = dbscan.labels_\n",
    "\n",
    "    # Apply t-SNE for dimensionality reduction with 2 components\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    topic_tsne = tsne.fit_transform(topic_proportions)\n",
    "\n",
    "    # Create DataFrame for visualization\n",
    "    df_tsne = pd.DataFrame(topic_tsne, columns=['tsne1', 'tsne2'])\n",
    "    df_tsne['Cluster'] = cluster_labels\n",
    "\n",
    "    # Plot clusters\n",
    "    plt.subplot(2, 3, i)\n",
    "    sns.scatterplot(data=df_tsne, x='tsne1', y='tsne2', hue='Cluster', palette='viridis', alpha=0.8)\n",
    "    plt.title(f'DBSCAN Clustering (epsilon={epsilon})')\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.legend(title='Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Range of min_samples values to try\n",
    "min_samples_values = [10,15,20,25]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, min_samples in enumerate(min_samples_values, 1):\n",
    "    # Initialize DBSCAN with the specified min_samples value and epsilon=0.1\n",
    "    dbscan = DBSCAN(eps=0.1, min_samples=min_samples)\n",
    "    dbscan.fit(topic_proportions)\n",
    "\n",
    "    # Get cluster labels\n",
    "    cluster_labels = dbscan.labels_\n",
    "\n",
    "    # Apply t-SNE for dimensionality reduction with 2 components\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    topic_tsne = tsne.fit_transform(topic_proportions)\n",
    "\n",
    "    # Create DataFrame for visualization\n",
    "    df_tsne = pd.DataFrame(topic_tsne, columns=['tsne1', 'tsne2'])\n",
    "    df_tsne['Cluster'] = cluster_labels\n",
    "\n",
    "    # Plot clusters\n",
    "    plt.subplot(2, 2, i)\n",
    "    sns.scatterplot(data=df_tsne, x='tsne1', y='tsne2', hue='Cluster', palette='viridis', alpha=0.8)\n",
    "    plt.title(f'DBSCAN Clustering (epsilon=0.1, min_samples={min_samples})')\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.legend(title='Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Range of min_samples values to try\n",
    "min_samples_values = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, min_samples in enumerate(min_samples_values, 1):\n",
    "    print(i)\n",
    "    # Initialize DBSCAN with the specified min_samples value and epsilon=0.1\n",
    "    dbscan = DBSCAN(eps=0.1, min_samples=min_samples)\n",
    "    dbscan.fit(topic_proportions)\n",
    "\n",
    "    # Get cluster labels\n",
    "    cluster_labels = dbscan.labels_\n",
    "\n",
    "    # Apply t-SNE for dimensionality reduction with 3 components\n",
    "    tsne = TSNE(n_components=3, random_state=42)\n",
    "    topic_tsne_3d = tsne.fit_transform(topic_proportions)\n",
    "\n",
    "    # Create DataFrame for visualization\n",
    "    df_tsne_3d = pd.DataFrame(topic_tsne_3d, columns=['tsne1', 'tsne2', 'tsne3'])\n",
    "    df_tsne_3d['Cluster'] = cluster_labels\n",
    "\n",
    "    # Plot clusters\n",
    "    ax = plt.subplot(2, 2, i, projection='3d')\n",
    "    for cluster in np.unique(cluster_labels):\n",
    "        if cluster == -1:\n",
    "            ax.scatter(df_tsne_3d.loc[df_tsne_3d['Cluster'] == cluster, 'tsne1'],\n",
    "                       df_tsne_3d.loc[df_tsne_3d['Cluster'] == cluster, 'tsne2'],\n",
    "                       df_tsne_3d.loc[df_tsne_3d['Cluster'] == cluster, 'tsne3'],\n",
    "                       label=f'Cluster {cluster}', c='black', alpha=0.8)\n",
    "        else:\n",
    "            ax.scatter(df_tsne_3d.loc[df_tsne_3d['Cluster'] == cluster, 'tsne1'],\n",
    "                       df_tsne_3d.loc[df_tsne_3d['Cluster'] == cluster, 'tsne2'],\n",
    "                       df_tsne_3d.loc[df_tsne_3d['Cluster'] == cluster, 'tsne3'],\n",
    "                       label=f'Cluster {cluster}', alpha=0.8)\n",
    "\n",
    "    ax.set_title(f'DBSCAN Clustering (epsilon=0.1, min_samples={min_samples})')\n",
    "    ax.set_xlabel('t-SNE Component 1')\n",
    "    ax.set_ylabel('t-SNE Component 2')\n",
    "    ax.set_zlabel('t-SNE Component 3')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
